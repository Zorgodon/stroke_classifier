{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Notebook 2 we know that specific hyperparameters\n",
    "\n",
    "Leaf size = 1 \n",
    "\n",
    "p = 1 (Manhattan distance)\n",
    "\n",
    "Nieghbour number = 7\n",
    "\n",
    "From this we can try doing the regessor object on data that has specific variables removed, by doing this we can understand which variables are the most important to the accuracy of the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib nbagg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import matplotlib.widgets\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.widgets import RadioButtons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing old classes\n",
    "\n",
    "Here we reimport old classes that will be used in the functions below. Namely preprocessor, regressor and postplotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to preprocess data\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    # initalise self with data needed to be preproccessed\n",
    "    def __init__ (self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def encode(self, kind: str, c_name:str ):\n",
    "        \"\"\"encoder(kind:str, c_name)\n",
    "           kind  is either onehot or label\n",
    "           c_name is the name of the column that will be encoded\n",
    "           \"\"\"\n",
    "        # update self.data with the appropriate encoder\n",
    "        assert kind in ['onehot', 'label']\n",
    "        self.kind = kind\n",
    "          \n",
    "        # do one hot encoder\n",
    "        if self.kind == 'onehot':\n",
    "            classes = set(self.data[c_name])\n",
    "            for cls in classes:\n",
    "                self.data[cls] = self.data[c_name].apply(lambda x: 1 if x == cls else 0)\n",
    "            del self.data[c_name]\n",
    "            return self.data\n",
    "            \n",
    "        # do label encoder\n",
    "        else:\n",
    "            classes = set(self.data[c_name])\n",
    "            values = {list(classes)[i]: i for i in range(len(classes))}\n",
    "            self.data[c_name] = self.data[c_name].apply(lambda x: values[x])\n",
    "\n",
    "        return self.data\n",
    "    \n",
    "    def scale(self, kind: str, c_name: str):\n",
    "        \"\"\"scale(kind:str, c_name)\n",
    "           kind  is either minmax or std\n",
    "           c_name is the name of the column that will be scaled\"\"\"\n",
    "            \n",
    "        # overwrites columns in self.data with \n",
    "        # scaled versions based on kind parameter\n",
    "        \n",
    "        # make sure kind is either minmax or std\n",
    "        assert kind in ['minmax', 'std']\n",
    "\n",
    "        # start with minmax\n",
    "        if kind == 'minmax':\n",
    "            min_value = min(self.data[c_name])\n",
    "            max_value = max(self.data[c_name])\n",
    "            diff = max_value - min_value\n",
    "            self.data[c_name] = self.data[c_name].apply(lambda x: (x - min_value) / diff)\n",
    "            \n",
    "            return self.data\n",
    "        \n",
    "        # then do standard scaler\n",
    "        elif kind == 'std':\n",
    "            mean = self.data[c_name].mean()\n",
    "            std = self.data[c_name].std()\n",
    "            self.data[c_name] = self.data[c_name].apply(lambda x: (x - mean) / std)\n",
    "        else:\n",
    "            print('Invalid scaler, check parameters before proceeding with ML')\n",
    "            \n",
    "        return self.data\n",
    "\n",
    "    # function to remove entire column of data\n",
    "    def remove (self, c_name):\n",
    "        del self.data[c_name]\n",
    "        return self.data\n",
    "     \n",
    "    # function to replace unknown data with mean, median or dropping it\n",
    "    def replace (self, c_name, kind):\n",
    "        assert kind in ['mean', 'median','drop']\n",
    "        \n",
    "        if kind == 'mean':\n",
    "            mean_value = self.data[c_name].mean()\n",
    "            self.data[c_name] = self.data[c_name].fillna(mean_value)\n",
    "            return self.data\n",
    "        \n",
    "        if kind == 'median':\n",
    "            median_value = self.data[c_name].median()\n",
    "            self.data[c_name] = self.data[c_name].fillna(median_value)\n",
    "            return self.data\n",
    "            \n",
    "        else:\n",
    "            self.data = self.data.dropna().reset_index(drop = True)\n",
    "            return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(fname: str, cv: str) -> tuple:\n",
    "    # basic spliter for now\n",
    "    \"\"\"Loads 2 column data from a CSV file with 2 columns, x and y\"\"\"\n",
    "\n",
    "    # read data\n",
    "    data = pd.read_csv(fname)\n",
    "    y_column = 'stroke'\n",
    "    # get training data\n",
    "    #n_rows = int(len(data) * training_ratio)\n",
    "\n",
    "    X_columns = list(data.columns)\n",
    "    X_columns.remove(y_column)\n",
    "    X = data[X_columns].to_numpy()\n",
    "    y = data[y_column].to_numpy()\n",
    "\n",
    "    # splitting calling the scikit split method of each cross validator\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # return all\n",
    "    return X, y, X_train, y_train, X_test, y_test\n",
    "\n",
    "class Regressor:\n",
    "    \"\"\"Container for analysing different metrics for a single regression class\"\"\"\n",
    "    def __init__(self, cls, fname: str, cv, cls_kwargs: dict = {}, cv_kwargs: dict = {} ):\n",
    "        # construct regressor object\n",
    "        self.regressor = cls(**cls_kwargs)\n",
    "        self.cv = cv(**cv_kwargs)\n",
    "\n",
    "        # use load function\n",
    "        # where cv is a splitting class i.e kFold()\n",
    "        self.X, self.y, self.X_train, self.y_train, self.X_test, self.y_test = load(fname, self.cv)\n",
    "\n",
    "        # fit data\n",
    "        self.regressor.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # get predicted data\n",
    "        self.y_pred = self.regressor.predict(self.X_test)\n",
    "\n",
    "    def metric(self, cls, **kwargs) -> float:\n",
    "        \"\"\"Takes a sklearn.metrics class and returns the score of the regressor object\"\"\"\n",
    "\n",
    "        # use the metric class to get a score\n",
    "        return cls(self.y_test, self.y_pred)\n",
    "    \n",
    "    def predict(self, newdata):\n",
    "        y = self.regressor.predict(newdata)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class postplotter:\n",
    "    \n",
    "    #initalise self with regressor object and test data\n",
    "    def __init__ (self, x_test, y_test, y_pred):\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        self.x_test = x_test\n",
    "        \n",
    "    # plots confusion matrix and gives FR values\n",
    "        \n",
    "    def confmatrix (self, title: str):\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        array = np.zeros((2, 2))\n",
    "\n",
    "        for i, correct_value in enumerate(self.y_test):\n",
    "            predicted_value = int(abs(np.round(self.y_pred[i])))\n",
    "            array[predicted_value, correct_value] += 1\n",
    "        \n",
    "\n",
    "\n",
    "        # plot array as image\n",
    "        im = ax.imshow(array, origin='lower', cmap='viridis')\n",
    "\n",
    "        # label axes\n",
    "        ax.set_xlabel('Real Type')\n",
    "        ax.set_ylabel('Predicted Type')\n",
    "\n",
    "        # add colorbar\n",
    "        plt.colorbar(im)\n",
    "        \n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "\n",
    "                # leave colour as white as the default\n",
    "                color = 'black'\n",
    "                if (i == 2 and j != 2):\n",
    "\n",
    "                    # when overdose happened but wasn't predicted, changed colour\n",
    "                    # to red if it is not 0\n",
    "                    if array[i, j] == 0:\n",
    "                        color = 'black'\n",
    "\n",
    "                    else:\n",
    "                        color = 'black'\n",
    "\n",
    "                # set colour to black for the light coloured square\n",
    "                # this is done after plotting once\n",
    "                elif i == 1 and j == 1:\n",
    "                    color = 'black'\n",
    "\n",
    "                # add text with correct colour\n",
    "                ax.text(j, i, array[i, j], ha='center', va='center', color=color)\n",
    "        \n",
    "        #sets axis title\n",
    "        ax.set_title(title, fontsize=15)\n",
    "\n",
    "        \n",
    "        #calculates F1, P and R values\n",
    "        R = (array[1,1]/(array[1,1]+array[1,0]))*100\n",
    "        \n",
    "        if (array[1,1]+array[0,1]) != 0:\n",
    "            P = (array[1,1]/(array[1,1]+array[0,1]))*100\n",
    "            F1 = 2*P*R/(P+R)\n",
    "            confresults = pd.DataFrame({'Precision P': P, 'F-score F1': F1, 'Recall R': R }, index=[0])\n",
    "\n",
    "        else:\n",
    "            confresults = pd.DataFrame({'Precision P': 'N/A', 'F-score F1': 'N/A', 'Recall R': R }, index=[0])\n",
    "            print(\"P and F1 could not be calculated due to 0 total correct guesses\")\n",
    "\n",
    "        print(confresults.to_string(index=False))\n",
    "        return P, F1, R\n",
    "    \n",
    "    # changed confusion matrix function to output metrics as well for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column removal\n",
    "\n",
    "Here we removed columns from the preprocessed data, to be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csvs for see effect of removing each column using radio buttons\n",
    "# read preprocessed data to delete columns to used in the final cell, hence output to csv\n",
    "\n",
    "# load preporcessed data\n",
    "data = pd.read_csv('data/splits/split21.csv')\n",
    "\n",
    "# run loop over names\n",
    "for name in data.columns:\n",
    "    \n",
    "    # generate class using copy of data so it can be reused in loop\n",
    "    datap = Preprocessor(data.copy())\n",
    "    # remove specific data column\n",
    "    datare = datap.remove(name)\n",
    "    # output to csv in diff file \n",
    "    datare.to_csv(f'data/data_removed/{name}_removed_split21.csv', index=False)\n",
    "# This is the final preprocessd data. CSVs have been generated with each column and the stroke column\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score generation\n",
    "\n",
    "Here we use the confusion matrix function to get the scores of the data. Namely, percision, recall and F1 score. This is complied into a list for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import full data set\n",
    "data = pd.read_csv('data/splits/split21.csv')\n",
    "\n",
    "# create kwargs dictionary\n",
    "cls_kwargs = { \"n_neighbors\":7 , \"leaf_size\":1 , \"p\":1 }\n",
    "\n",
    "# create scores dictionary to store plot data\n",
    "scores = {}\n",
    "\n",
    "# use newly found optimised hyperparameters to make new regressor item\n",
    "full_reg = Regressor(KNeighborsClassifier,'data/splits/split21.csv', StratifiedKFold, cls_kwargs)\n",
    "plot_full = postplotter(full_reg.X_test, full_reg.y_test, full_reg.y_pred)\n",
    "# append to score with title 'full'\n",
    "scores['full'] = plot_full.confmatrix(\"All data\")\n",
    "\n",
    "# cycle over all removed column datasets\n",
    "for name in data.columns:\n",
    "    # don't remove stroke data\n",
    "    if name == \"stroke\":  \n",
    "        pass\n",
    "    else:\n",
    "        # make a temporary regressor item with the removed csv file\n",
    "        temp_reg = Regressor(KNeighborsClassifier,f'data/data_removed/{name}_removed_split21.csv', StratifiedKFold)\n",
    "        plot_full = postplotter(temp_reg.X_test, temp_reg.y_test, temp_reg.y_pred)\n",
    "        # append to score with title of removed data\n",
    "        scores[f'{name}'] = plot_full.confmatrix(f'{name} removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive graph\n",
    "\n",
    "Here we make an interactive graph for the user to see what parameters of the data matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and move subplot\n",
    "fig, ax = plt.subplots()\n",
    "plt.subplots_adjust(left=0.45)\n",
    "\n",
    "# create colour dictionary to allow changing of bar chart colour.\n",
    "color = ['blue','orange','green','red','m','pink','cyan','gray']\n",
    "colors = {}\n",
    "for  i , name in enumerate (scores):\n",
    "    colors[name] = color[i]\n",
    "\n",
    "# define bottom of the the bar chart\n",
    "objects = ('Precision' , 'F-score' ,  'Recall' )\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "# makes y bar go to 100%\n",
    "ax.set_ylim([0,100])\n",
    "ax.set_ylabel('%')\n",
    "\n",
    "# pull from dictionary for inital\n",
    "performance = scores[\"full\"]\n",
    "\n",
    "# plot the bar chart\n",
    "ax.bar(y_pos, performance, align='center')\n",
    "\n",
    "# change ticks to titles\n",
    "plt.xticks(y_pos, objects)\n",
    "\n",
    "\n",
    "# create radio box\n",
    "radio_ax = plt.axes( [0.0, 0.45, 0.3, 0.3],  facecolor='#ffffff')\n",
    "# create radio buttons\n",
    "radio = RadioButtons( radio_ax, scores.keys())\n",
    "\n",
    "# callback function to run when buttons are pressed\n",
    "def callback(label: str):\n",
    "    \n",
    "    # clear plot\n",
    "    ax.clear()\n",
    "    \n",
    "    # reset all objects\n",
    "    ax.set_ylim([0,100])\n",
    "    ax.set_ylabel('%')\n",
    "    performance = scores[label]\n",
    "    ax.set_xticks(y_pos)\n",
    "    ax.set_xticklabels(objects)\n",
    "    \n",
    "    # plot new data\n",
    "    ax.bar(y_pos, performance, align='center', color = colors[label])\n",
    "\n",
    "# run on radio click\n",
    "radio.on_clicked(callback)\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key findings\n",
    "\n",
    "From the interactive graph we can see that age is highly relied upon by the regressor, this is expected as it is known that stroke chance increases with age.\n",
    "Moverover, hypertension and heart disease give the same output, this is likely due to them being closely related to each other. And hence effect the regressor the same way when removed.\n",
    "Marriage removal increases precision but reduces recall, calling in a similar F-score.\n",
    "All avg glucose level, bmi and smoking status reduce all 3 scores of the data.\n",
    "\n",
    "The fact that removing any variable reduces the predictivity of the regresssor suggests that our data selection was correct if not over zealous, as the best regressor is made from our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
