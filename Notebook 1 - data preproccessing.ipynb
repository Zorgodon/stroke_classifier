{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preproccessing and classifier implementation \n",
    "\n",
    "In this notebook, data will be preprocessed and output as csv files to be used in the notebooks afterwards.\n",
    "A classifier is then fitted from data split into test and training sets.\n",
    "\n",
    "For the processes to run, 2 new folders need to be generated \"data\" and \"data_removed\" nested in \"data\".\n",
    "This allows all the data to be preprocessed in one cell and be called from the other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "#FIXME: array[1] vs correct_values\n",
    "#FIXME: split_21 justify/change\n",
    "#FIXME: remove from remove_list\n",
    "\n",
    "%matplotlib nbagg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "import matplotlib.animation\n",
    "import matplotlib.widgets\n",
    "#import seaborn as sns\n",
    "\n",
    "#from tkinter import * \n",
    "#import tkinter.messagebox \n",
    "\n",
    "#from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.widgets import RadioButtons\n",
    "#from sklearn import svm\n",
    "from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n",
    "                                     StratifiedKFold, GroupShuffleSplit,\n",
    "                                     GroupKFold, StratifiedShuffleSplit)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessor class\n",
    "\n",
    "A class is made to preprocess data so these lines do not need to be call multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to preprocess data\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    # initalise self with data needed to be preproccessed\n",
    "    def __init__ (self, data):\n",
    "        \"\"\"Initalise class with pandas dataframe.\"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def encode(self, kind: str, c_name:str ):\n",
    "        \"\"\"encoder(kind:str, c_name)\n",
    "           kind  is either 'onehot' or 'label'\n",
    "           c_name is the name of the column that will be encoded as a string\n",
    "           \"\"\"\n",
    "        \n",
    "        # update self.data with the appropriate encoder\n",
    "        # ensure input is a column of dataframe \n",
    "        assert kind in ['onehot', 'label']\n",
    "        assert c_name in self.data.columns\n",
    "        self.kind = kind\n",
    "          \n",
    "        # do one hot encoder\n",
    "        if self.kind == 'onehot':\n",
    "            classes = set(self.data[c_name])\n",
    "            for cls in classes:\n",
    "                self.data[cls] = self.data[c_name].apply(lambda x: 1 if x == cls else 0)\n",
    "            del self.data[c_name]\n",
    "            return self.data\n",
    "            \n",
    "        # do label encoder\n",
    "        else:\n",
    "            classes = set(self.data[c_name])\n",
    "            values = {list(classes)[i]: i for i in range(len(classes))}\n",
    "            self.data[c_name] = self.data[c_name].apply(lambda x: values[x])\n",
    "\n",
    "        return self.data\n",
    "    \n",
    "    def scale(self, kind: str, c_name: str):\n",
    "        \"\"\"scale(kind:str, c_name)\n",
    "           kind  is either minmax or std\n",
    "           c_name is the name of the column that will be scaled as a string\n",
    "           \"\"\"\n",
    "            \n",
    "        # overwrites columns in self.data with \n",
    "        # scaled versions based on kind parameter\n",
    "        \n",
    "         # make sure kind is either minmax or standard\n",
    "        # ensure input is a column of dataframe\n",
    "        assert kind in ['minmax', 'standard']\n",
    "        assert c_name in self.data.columns\n",
    "\n",
    "        # start with minmax\n",
    "        if kind == 'minmax':\n",
    "            min_value = min(self.data[c_name])\n",
    "            max_value = max(self.data[c_name])\n",
    "            diff = max_value - min_value\n",
    "            self.data[c_name] = self.data[c_name].apply(lambda x: (x - min_value) / diff)\n",
    "            \n",
    "            return self.data\n",
    "        \n",
    "        # then do standard scaler\n",
    "        else:\n",
    "            mean = self.data[c_name].mean()\n",
    "            std = self.data[c_name].std()\n",
    "            self.data[c_name] = self.data[c_name].apply(lambda x: (x - mean) / std)\n",
    "            \n",
    "        return self.data\n",
    "\n",
    "    # function to remove entire column of data\n",
    "    def remove (self, c_name):\n",
    "        \"\"\"Removes an entire column of data, for uncorrelated data takes\n",
    "         column key as string.\"\"\"\n",
    "        \n",
    "        # ensure input is a column of dataframe\n",
    "        assert c_name in self.data.columns\n",
    "\n",
    "        del self.data[c_name]\n",
    "        return self.data\n",
    "     \n",
    "    # function to replace unknown data with mean, median or dropping it\n",
    "    def replace (self, c_name, kind):\n",
    "        \"\"\"Replaces missing data by mean, median or dropping it, \n",
    "        takes replacement type and column key as string.\"\"\"\n",
    "        \n",
    "        # ensure kind is a type of missing data remover\n",
    "        # ensure input is a column of dataframe\n",
    "        assert kind in ['mean', 'median','drop']\n",
    "        assert c_name in self.data.columns\n",
    "        \n",
    "        if kind == 'mean':\n",
    "            mean_value = self.data[c_name].mean()\n",
    "            self.data[c_name] = self.data[c_name].fillna(mean_value)\n",
    "            return self.data\n",
    "        \n",
    "        if kind == 'median':\n",
    "            median_value = self.data[c_name].median()\n",
    "            self.data[c_name] = self.data[c_name].fillna(median_value)\n",
    "            return self.data\n",
    "            \n",
    "        else:\n",
    "            self.data = self.data.dropna().reset_index(drop = True)\n",
    "            return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotter class\n",
    "\n",
    "Plotter class was used to make scatter and line graphs, this was done to make the class as generic as possible to allow different types of data to be plotted. The line graphs were not used give as the data output was boolean, but kept in the code.\n",
    "\n",
    "This also plots 3D graphs and radiobuttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preplotter class to help with inital data visualisation\n",
    "\n",
    "class preplotter:\n",
    "    \n",
    "    def __init__ (self, data):\n",
    "        \"\"\"Initalise class with pandas dataframe.\"\"\"\n",
    "        self.data = data\n",
    "        \n",
    "    # Function to plot linear 2D or 3D graph takes in column names and output graphs\n",
    "        \n",
    "    # func to plots scatter graphs \n",
    "    # for these functions column names are input, hence by setting the z name to 0 and checking it is not a string\n",
    "    # we can seperate 3D plots from 2D plots\n",
    "    \n",
    "    # for all the functions below, they follow a similar manner\n",
    "    # get fig ax objects\n",
    "    # label axis\n",
    "    # plot graph \n",
    "    \n",
    "    def scatter(self, x_name, y_name, z_name = 0 ):\n",
    "        \"\"\"Plots scatter graphs of either 2 or 3 single vairables, all column keys as string.\"\"\"\n",
    "\n",
    "        \n",
    "        # check if data has 3rd dimension\n",
    "        if z_name == 0:\n",
    "            \n",
    "            # ensure axis are in dataset\n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "            \n",
    "            # gen fig and ax for 2D\n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "            # label fig and ax\n",
    "            ax.set_xlabel(x_name)\n",
    "            ax.set_ylabel(y_name)\n",
    "            \n",
    "            # plot data\n",
    "            ax.scatter(self.data[x_name], self.data[y_name], alpha=0.5)\n",
    "        \n",
    "        # z_name != 0, therefore we plot in 3D\n",
    "        # generate and for above but in 3D graph\n",
    "        else:\n",
    "            \n",
    "            # ensure axis are in dataset\n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "            assert z_name in self.data.columns\n",
    "            \n",
    "            # gen fig and ax for 3D\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            \n",
    "            # label fig and ax\n",
    "            ax.set_xlabel(x_name)\n",
    "            ax.set_ylabel(y_name)\n",
    "            ax.set_zlabel(z_name)\n",
    "            \n",
    "            # plot data\n",
    "            ax.scatter(self.data[x_name], self.data[y_name], self.data[z_name])    \n",
    "    \n",
    "    ### func to plot line graphs same as above with \".scatter\" -> \".plot\"\n",
    "    \n",
    "    def line(self, x_name, y_name, z_name = 0):\n",
    "        \"\"\"Plots line graphs of either 2 or 3 single vairables, all column keys as string.\"\"\"\n",
    "\n",
    "        if z_name == 0:\n",
    "            \n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "            \n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "            ax.set_xlabel(x_name)\n",
    "            ax.set_ylabel(y_name)\n",
    "            \n",
    "            ax.plot(self.data[x_name], self.data[y_name])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "            assert z_name in self.data.columns\n",
    "            \n",
    "            fig = plt.figure()\n",
    "\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            \n",
    "            ax.set_xlabel(x_name)\n",
    "            ax.set_ylabel(y_name)\n",
    "            ax.set_zlabel(z_name)\n",
    "            \n",
    "            ax.plot(self.data[x_name], self.data[y_name], self.data[z_name])\n",
    "            \n",
    "    # plot multiple line plots with radio buttons\n",
    "    \n",
    "    def multiline (self, x_list , y_list, z_list = 0):\n",
    "        \"\"\"Plot multiple sets of data in a list against each other as a line graph, takes list of each \n",
    "        axis with respect to each other, radio buttons only work for 2 lists\"\"\"\n",
    "        \n",
    "        #hard set colours to be used, don't expect more than 8 values to be plotted at once\n",
    "        colours = ('blue', 'green', 'orange', 'red', 'yellow' , 'purple', 'black', 'pink')\n",
    "\n",
    "        # check if input has 3 dimension\n",
    "        if z_list == 0:\n",
    "\n",
    "            # gen fig, ax adjusted for radio buttons\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.subplots_adjust(left=0.45)\n",
    "\n",
    "            # generate empty dictionary\n",
    "            lines = {}\n",
    "            \n",
    "            # generate line dictionary using x_list as keys\n",
    "            for i ,name in enumerate(x_list):\n",
    "\n",
    "                if i == 0:\n",
    "                    lines[name] = ax.plot( self.data[x_list[i]], self.data[y_list[i]], c=colours[i], alpha=0.5)\n",
    "\n",
    "                else:\n",
    "                    lines[name] = ax.plot( self.data[x_list[i]], self.data[y_list[i]], c=colours[i], alpha=0.025)\n",
    "\n",
    "            # create radio box\n",
    "            radio_ax = plt.axes( [0.0, 0.45, 0.3, 0.3],  facecolor='#ffffff')\n",
    "\n",
    "            # create radio buttons\n",
    "            radio = RadioButtons( radio_ax, x_list)\n",
    "\n",
    "            # callback function to run when buttons are pressed\n",
    "            def callback(label: str):\n",
    "                for name, line in lines.items():\n",
    "                    if name == label:\n",
    "                        line[0].set_alpha(0.5)\n",
    "                    else:\n",
    "                        line[0].set_alpha(0.025)\n",
    "                plt.draw()\n",
    "                return\n",
    "\n",
    "            # connect function to radio object and show\n",
    "            radio.on_clicked(callback)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            return radio\n",
    "\n",
    "\n",
    "        # if in 3D do simple colour plot\n",
    "        else:\n",
    "            \n",
    "            # ensure axis are in dataset\n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "            assert z_name in self.data.columns\n",
    "            \n",
    "            # generate fig and ax\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            \n",
    "            # plot over all data provided\n",
    "            for i in range(len(x_list)):\n",
    "                ax.plot( self.data[x_list[i]], self.data[y_list[i]], self.data[z_list[i]], label= x_list[i], alpha = 0.5)\n",
    "                plt.legend()\n",
    "\n",
    "\n",
    "    ### func to plot line graphs same as above with \".plot\" -> \".scatter\"\n",
    "    \n",
    "    def multiscatter (self, x_list , y_list, z_list = 0):\n",
    "        \"\"\"Plot multiple sets of data in a list against each other as a scatter graph, takes list of each \n",
    "        axis with respect to each other, radio buttons only work for 2 lists\"\"\"\n",
    "\n",
    "        colours = ('blue', 'green', 'orange', 'red', 'yellow' , 'purple', 'black', 'pink')\n",
    "\n",
    "        if z_list == 0:\n",
    "            \n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.subplots_adjust(left=0.45)\n",
    "\n",
    "            lines = {}\n",
    "\n",
    "            for i ,name in enumerate(x_list):\n",
    "\n",
    "                if i == 0:\n",
    "                    lines[name] = ax.scatter( self.data[x_list[i]], self.data[y_list[i]], c=colours[i], alpha=0.5)\n",
    "\n",
    "                else:\n",
    "                    lines[name] = ax.scatter( self.data[x_list[i]], self.data[y_list[i]], c=colours[i], alpha=0.025)\n",
    "\n",
    "            # create radio  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1box\n",
    "            radio_ax = plt.axes( [0.0, 0.45, 0.3, 0.3],  facecolor='#ffffff')\n",
    "\n",
    "            # create radio buttons\n",
    "            radio = RadioButtons( radio_ax, x_list)\n",
    "\n",
    "            # callback function to run when buttons are pressed\n",
    "            def callback(label: str):\n",
    "                for name, line in lines.items():\n",
    "                    if name == label:\n",
    "                        line.set_alpha(0.5)\n",
    "                    else:\n",
    "                        line.set_alpha(0.025)\n",
    "                plt.draw()\n",
    "                return\n",
    "\n",
    "            # connect function to radio object and show\n",
    "            radio.on_clicked(callback)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            return radio\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            assert x_name in self.data.columns\n",
    "            assert y_name in self.data.columns\n",
    "            assert z_name in self.data.columns\n",
    "            \n",
    "            fig = plt.figure()\n",
    "\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            for i in range(len(x_list)):\n",
    "                ax.scatter( self.data[x_list[i]], self.data[y_list[i]], self.data[z_list[i]], label= x_list[i], alpha = 0.5)\n",
    "                plt.legend()\n",
    "\n",
    "                \n",
    "    # this function plots a boolean data type's coordination with strokes\n",
    "    def histogram (self, x_list):\n",
    "        \"\"\"Specific to this dataset, the function takes in a single boolean argument as a string and \n",
    "        plots the difference in stroke chance between them, returning the difference.\"\"\"\n",
    "\n",
    "        # initalise figure and bins\n",
    "        fig, ax = plt.subplots()\n",
    "        bins = [0 ,1]\n",
    "        \n",
    "        # filter data out based on if boolean is 1 or 0\n",
    "        \n",
    "        inter = self.data[self.data[x_list] == 1]\n",
    "        # check amount of strokes for bool of 1\n",
    "        strokes = len(inter[inter['stroke'] == 1])\n",
    "        sstrokes = len(inter)\n",
    "        # find % strokes\n",
    "        pstrokes = (strokes/sstrokes)*100\n",
    "        print(f'the percentage of strokes with 1 is {pstrokes}%.')\n",
    "        \n",
    "        # repear with bool of 0\n",
    "        inter2 = self.data[self.data[x_list] == 0]\n",
    "        nstrokes = len(inter2[inter2['stroke'] == 1])\n",
    "        snstrokes = len(inter2)\n",
    "        pnstrokes = (nstrokes/snstrokes)*100\n",
    "        print(f'the percentage of strokes with 0 is {pnstrokes}%.')\n",
    "        \n",
    "        # plot these 2 percentage chances against each other and print difference\n",
    "        score = [pnstrokes, pstrokes]\n",
    "        ax.bar(bins,score)\n",
    "        dv = pstrokes - pnstrokes\n",
    "    \n",
    "        print(f'the percentage difference is {dv}%.')\n",
    "        \n",
    "        return dv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "\n",
    "Here we do the 1st stage of data preproccessing using basic common sense, such as \"id\" being an arbitary hospital allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 1st stage data preprocessing by logic\n",
    "\n",
    "data = pd.read_csv('data/healthcare-dataset-stroke-data.csv')\n",
    "# initialise preporcessor class\n",
    "preprocessor = Preprocessor(data.copy())\n",
    "\n",
    "# known id is irrelavent to stroke chance\n",
    "preprocessor.remove('id')\n",
    "\n",
    "# there is enough data to justify dropping NaN values in bmi\n",
    "preprocessor.replace('bmi','drop')\n",
    "\n",
    "# there are only 2 people with other as a gender, as insensitive as this is, they can be dropped\n",
    "preprocessor.encode('onehot','gender')\n",
    "preprocessor.remove(\"Other\")\n",
    "\n",
    "# there are only 2 marriage options, no live in partners or polygamy smh\n",
    "preprocessor.encode('label','ever_married')\n",
    "\n",
    "# more than 2 work tpyes - > onehot\n",
    "preprocessor.encode('onehot','work_type')\n",
    "\n",
    "# there are only 2 residencital types and one is obviously better than the other\n",
    "preprocessor.encode('label','Residence_type')\n",
    "\n",
    "# label encoding for smoking a hierarchy makes sense in terms of stroke risk\n",
    "preprocessor.encode('label','smoking_status')\n",
    "\n",
    "# all other data is between 1 and 0 as such to keep all data compareable, we should use minmax scaling as it also keeps\n",
    "# values within 1 and 0\n",
    "help(preprocessor.encode)\n",
    "help(preprocessor.scale)\n",
    "preprocessor.scale('minmax','age')\n",
    "preprocessor.scale('minmax','avg_glucose_level')\n",
    "preprocessor.scale('minmax','bmi')\n",
    "\n",
    "#output data to another csv file\n",
    "#preprocessor.data.to_csv('data/preprocessed-healthcare-dataset-stroke-data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "\n",
    "Here we remove boolean data based on the difference in the stroke chance if the population where bool = 1, and bool = 0. If there is a corrolation, there will be a difference in the stroke chance. Here we set an arbitary cutoff at 1.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 2nd stage data preprocessing by correlation with stroke chance\n",
    "\n",
    "# get all keys from datasheet\n",
    "bols = []\n",
    "for key in preprocessor.data.columns:\n",
    "    bols.append(key)\n",
    "\n",
    "# Columns in the dataset were either boolean (0 or 1) or scaled values\n",
    "# This processor part only considers correlation between boolean values\n",
    "# and strokes\n",
    "\n",
    "# manually set non-bool keys acceptable for small column list\n",
    "nbols = ['age','avg_glucose_level','bmi', 'smoking_status']\n",
    "\n",
    "# so non-booleans are removed for this part:\n",
    "for i in nbols:\n",
    "    bols.remove(i)\n",
    "\n",
    "bols.remove('stroke')\n",
    "\n",
    "#FIXME: how are we handling smoking_status again?\n",
    "\n",
    "# innitialse class\n",
    "preplots = preplotter(preprocessor.data)\n",
    "\n",
    "# create a list containing columns that will not have ML performed on them\n",
    "badbols = ['Never_worked', 'children', 'Self-employed']\n",
    "#badbols = []\n",
    "\n",
    "# consider correlation betweeen having a positive (1) result for the columns\n",
    "# that are left and whether that person had a stroke\n",
    "for name in bols:\n",
    "    print(name)\n",
    "    # return difference in percentage stroke, if there is correlation there would be a significant difference in stroke\n",
    "    # amounts between the boolean anwsers\n",
    "    dv = preplots.histogram(name)\n",
    "    print('---------------------------------------------------------------------------------------------------------')\n",
    "    # remove data that has similar stroke percentage i.e. non-predictive of stroke chances\n",
    "    if abs(dv) < 1.1 : #ARBITRARY VALUE SET\n",
    "        badbols.append(name)\n",
    "\n",
    "print(badbols)\n",
    "\n",
    "# remove 2nd wave of non-predictive data\n",
    "for name in badbols:\n",
    "    preprocessor.remove(name)\n",
    "       \n",
    "# save to new file\n",
    "preprocessor.data.to_csv('data/preprocessed-healthcare-dataset-stroke-data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the plotter class to check whether the non-boolean data is correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print scatter graphs for each non-boolean function to check for corrolation\n",
    "#FIXME: matplotlib titles\n",
    "for name in nbols:\n",
    "    print(name)\n",
    "    preplots.scatter(name, 'stroke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a 3D scatter to show a clear correlation of age and bmi to stroke\n",
    "# visualise data in 3D\n",
    "#FIXME: plt.title()\n",
    "preplots.scatter('stroke','age','bmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column removal\n",
    "\n",
    "Here we removed columns from the preprocessed data, in preperation for the final notebook where the effects will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csvs for see effect of removing each column using radio buttons\n",
    "# read preprocessed data to delete columns to used in the final cell, hence output to csv\n",
    "\n",
    "# load preporcessed data\n",
    "data = pd.read_csv('data/preprocessed-healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "# run loop over names\n",
    "for name in data.columns:\n",
    "    \n",
    "    # generate class using copy of data so it can be reused in loop\n",
    "    datap = Preprocessor(data.copy())\n",
    "    # remove specific data column\n",
    "    datare = datap.remove(name)\n",
    "    # output to csv in diff file \n",
    "    datare.to_csv(f'data/data_removed/{name}_removed-healthcare-dataset-stroke-data.csv',)\n",
    "# This is the final preprocessd data. CSVs have been generated with each column and the stroke column\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the classifier\n",
    "In this section, we decide which of the classifiers to use and how to split our data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The regressor class\n",
    "This class is initialised with any sci kit learn regressor/classifier object, any sci kit cross validation object and a csv file with the data. It fits the regressor with data split using the cross validator and stores the resulting y_pred, X_test and y_test values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(fname: str, cv: str) -> tuple:\n",
    "    \"\"\"Loads data from a CSV file and splits it using scikit\"\"\"\n",
    "\n",
    "    # read data\n",
    "    data = pd.read_csv(fname)\n",
    "    y_column = 'stroke'\n",
    "    X_columns = list(data.columns)\n",
    "    X_columns.remove(y_column)\n",
    "    X = data[X_columns].to_numpy()\n",
    "    y = data[y_column].to_numpy()\n",
    "\n",
    "    # splitting calling the scikit split method of each cross validator\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # return all\n",
    "    return X, y, X_train, y_train, X_test, y_test\n",
    "\n",
    "class Regressor:\n",
    "    \"\"\"Container for analysing different metrics for a single regression class\"\"\"\n",
    "    def __init__(self, cls, fname: str, cv, cls_kwargs: dict = {}, cv_kwargs: dict = {} ):\n",
    "        # construct regressor object\n",
    "        self.regressor = cls(**cls_kwargs)\n",
    "        self.cv = cv(**cv_kwargs)\n",
    "\n",
    "        # use load function\n",
    "        # where cv is a splitting class i.e kFold()\n",
    "        self.X, self.y, self.X_train, self.y_train, self.X_test, self.y_test = load(fname, self.cv)\n",
    "\n",
    "        # fit data\n",
    "        self.regressor.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # get predicted data\n",
    "        self.y_pred = self.regressor.predict(self.X_test)\n",
    "\n",
    "    def metric(self, cls, **kwargs) -> float:\n",
    "        \"\"\"Takes a sklearn.metrics class and returns the score of the regressor object\"\"\"\n",
    "\n",
    "        # use the metric class to get a score\n",
    "        return cls(self.y_test, self.y_pred)\n",
    "    \n",
    "        # method that predicts new y values from new x data\n",
    "    def predict(self, newdata):\n",
    "        y = self.regressor.predict(newdata)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Postplotter class\n",
    "This class creates the confusion matrix to compare cross-validation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class postplotter:\n",
    "    \n",
    "    #initalise self with regressor object and test data\n",
    "    def __init__ (self, x_test, y_test, y_pred):\n",
    "        \"\"\"Initalise postplotter class with x test, y test and y prediction data.\"\"\"\n",
    "\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        self.x_test = x_test\n",
    "        \n",
    "    # plots confusion matrix and gives FR values\n",
    "        \n",
    "    def confmatrix (self, title: str):\n",
    "        \"\"\"Plots a confusion matrix using the data and gives percision, recall and F1 score, \n",
    "        takes title of plot as argument.\"\"\"\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        array = np.zeros((2, 2))\n",
    "\n",
    "        for i, correct_value in enumerate(self.y_test):\n",
    "            predicted_value = int(abs(np.round(self.y_pred[i])))\n",
    "            array[predicted_value, correct_value] += 1\n",
    "        \n",
    "\n",
    "\n",
    "        # plot array as image\n",
    "        im = ax.imshow(array, origin='lower', cmap='viridis')\n",
    "\n",
    "        # label axes\n",
    "        ax.set_xlabel('Real Type')\n",
    "        ax.set_ylabel('Predicted Type')\n",
    "\n",
    "        # add colorbar\n",
    "        plt.colorbar(im)\n",
    "        \n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "\n",
    "                # leave colour as white as the default\n",
    "                color = 'black'\n",
    "                if (i == 2 and j != 2):\n",
    "\n",
    "                    # when overdose happened but wasn't predicted, changed colour\n",
    "                    # to red if it is not 0\n",
    "                    if array[i, j] == 0:\n",
    "                        color = 'black'\n",
    "\n",
    "                    else:\n",
    "                        color = 'black'\n",
    "\n",
    "                # set colour to black for the light coloured square\n",
    "                # this is done after plotting once\n",
    "                elif i == 1 and j == 1:\n",
    "                    color = 'black'\n",
    "\n",
    "                # add text with correct colour\n",
    "                ax.text(j, i, array[i, j], ha='center', va='center', color=color)\n",
    "        \n",
    "        #sets axis title\n",
    "        ax.set_title(title, fontsize=15)\n",
    "\n",
    "        \n",
    "        #calculates F1, P and R values\n",
    "        R = (array[1,1]/(array[1,1]+array[1,0]))*100\n",
    "        \n",
    "        if (array[1,1]+array[0,1]) != 0:\n",
    "            P = (array[1,1]/(array[1,1]+array[0,1]))*100\n",
    "            F1 = 2*P*R/(P+R)\n",
    "            confresults = pd.DataFrame({'Precision P': P, 'F-score F1': F1, 'Recall R': R }, index=[0])\n",
    "\n",
    "        else:\n",
    "            confresults = pd.DataFrame({'Precision P': 'N/A', 'F-score F1': 'N/A', 'Recall R': R }, index=[0])\n",
    "            print(\"P and F1 could not be calculated due to 0 total correct guesses\")\n",
    "\n",
    "        print(confresults.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy issue \n",
    "When using the regressor on the entire data sets, classifiers found the best fit to predict 100% of the data to not have a stroke. The way we resolved this issue is by making the regressor run on a sample of the data where we use all the data on strokes and an equal amount of data for non strokes. This is justified below as we checked how this method performed across all the non-stroke data in 22 intervals of 210.\n",
    "\n",
    "Out of all the iterations, split 21 gave the best precision $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trying the KNeighbours classifier to justify using small amounts of the data to train the classifier \n",
    "daata = pd.read_csv('data/preprocessed2-healthcare-dataset-stroke-data.csv')\n",
    "strdata = daata.iloc[:210, :] #210 rows of stroke data\n",
    "for n in range(1,22): #22 lots of 210 in non-stroke data\n",
    "    rnddata = daata.iloc[210+n*210:210+(n+1)*210,:]\n",
    "    concat = strdata.append(rnddata)  \n",
    "    concat.to_csv(f'data/splits/split{n}.csv', index = False)\n",
    "    rg = Regressor(KNeighborsClassifier, f'data/splits/split{n}.csv', StratifiedKFold)\n",
    "    a = postplotter(rg.X_test, rg.y_test, rg.y_pred)\n",
    "    print(f'Iteration: {n}')\n",
    "    a.confmatrix('{}'.format(type(rg.regressor).__name__)+' \\n{}'.format(type(rg.cv).__name__ ) )\n",
    "    print(f'\\nClassifier R^2 Score: {rg.regressor.score(rg.X_test, rg.y_test)}\\n\\n-----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a classifier and splitting method\n",
    "In this cell we looped our regressor class over different CV and regression objects from scikit learn and compared the best F-scores to determine which we would use for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borrowed from the scikit learn documentation to visualise the different splitting techniques\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class', 'group']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Running different regressors with different splitting techniques and returns the confusion matrix in order\n",
    "#to determine which ones to use for the data \n",
    "reg_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier]\n",
    "cv_list = [ShuffleSplit, StratifiedKFold, KFold, StratifiedShuffleSplit]\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "#DEFINE FNAME #MASTER LOOP\n",
    "fname = 'data/splits/split21.csv'\n",
    "groups_ = data['stroke'].to_numpy()\n",
    "for regressor in reg_list: \n",
    "    for cv in cv_list:\n",
    "        rg = Regressor(regressor, fname, cv)\n",
    "        a = postplotter(rg.X_test, rg.y_test, rg.y_pred)\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_cv_indices(rg.cv, rg.X, rg.y, rg.y, ax, rg.cv.get_n_splits() )\n",
    "        ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(1.02, .8))\n",
    "\n",
    "        #alternatively: use regressor as string from reg_list\n",
    "        a.confmatrix( '{}'.format(type(rg.regressor).__name__)+' \\n{}'.format(type(cv()).__name__)  )\n",
    "        print(f'\\nClassifier R^2 Score: {rg.regressor.score(rg.X_test, rg.y_test)}\\n\\n-----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we decided it is best to use stratified Kfold so an even amount of the stroke data and non stroke data is used to fit the model. \n",
    "\n",
    "We chose to proceed with the K Nearest Neighbours classifier as it gave the highest $P$ and $R^2$.\n",
    "\n",
    "Once we decided to use these, the predicitivity was further analysed in Notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
